<!DOCTYPE HTML>
<!--
	Editorial by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>SGQuant</title>
		<link rel="icon" href="images/favicon.png" type="image/x-icon">
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<div id="main">
						<div class="inner">

							<!-- Header -->
								<header id="header">
									<!-- <a href="index.html" class="logo"><strong>Editorial</strong> by HTML5 UP</a> -->
									<ul class="icons">
										<!-- <li><a href="#" class="icon brands fa-twitter"><span class="label">Twitter</span></a></li> -->
<!-- 										<li><a href="https://www.facebook.com/amelia.peng.14" class="icon brands fa-facebook-f"><span class="label">Facebook</span></a></li> -->
<!-- 										<li><a href="https://github.com/Ameliapeng" class="icon brands fa-github"><span class="label">Github</span></a></li> -->
<!-- 										<li><a href="https://www.instagram.com/yuuuukii.131" class="icon brands fa-instagram"><span class="label">Instagram</span></a></li> -->
									</ul>
								</header>

							<!-- Content -->
								<section>
									<header class="main">
										<h1>SGQuant</h1>
									</header>
									<!-- <div class="posts">  -->
										<!-- <article>
											<span class="image main"><img src="images/IF_2.png" alt="" style="widows: 500px" /></span>
										</article> --> 
			
										<!-- <article> -->
											<!-- <span class="image main"><img src="images/IF_2.png" alt="" style="width: 500px; float: medium;"/></span> -->
											<!-- <span class="image main"> -->
											<!-- <img src="images/IF_1.png" alt="" style="width: 500px; float: right;" /></span> -->
										<!-- </article> -->
									<!-- </div> -->

									<img src="images/SGQuant_1.png" alt="" style="width: 400px; height: 300px;"/>
									<p>With the increasing popularity of graph-based learning, Graph Neural Networks (GNNs) win lots of attention from research and industry field because of their high accuracy. However, existing GNNs suffer from high memory footprints (e.g., node embedding features). This high memory footprint hurdles the potential applications towards memory-constrained devices, such as the widely-deployed IoT devices. To this end, we propose a specialized GNN quantization scheme, SGQuant, to systematically reduce the GNN memory consumption. Specifically, we first propose a GNN-tailored quantization algorithm design and a GNN quantization fine-tuning scheme to reduce memory consumption while maintaining accuracy. Then, we investigate the multi-granularity quantization strategy that operates at different levels (components, graph topology, and layers) of GNN computation. Moreover, we offer an automatic bit-selecting (ABS) to pinpoint the most appropriate quantization bits for the above multi-granularity quantizations. </p>
									

									<hr class="major" />

									<h2>Multi-granularity Quantization</h2>
									<span class="image major"></span><img src="images/SGQuant_2.png" alt="" style="width: 900px; height: 350px;"/></span>
									<p><br>We propose four different types of granularity: <b>(a)component-wise</b>, <b>(b)topology-aware</b>, <b>(c)layer-wise</b>, and <b>(d)uniform</b>, as illustrated in the figure. The simplest granularity is the uniform quantization, which applies the same quantization bits to all layers and components in the GNN. It helps reduce the memory consumption by replacing the 32-bit values with the corresponding q-bit quantized data representation. However, when applying the same quantization bit to all layers, nodes, and components, we ignore their different sensitivity to quantization bits and the introduced numerical error, leading to degraded accuracy. To this end, we need the quantization at finer granularity to cater the different sensitivity.</p>
									<p><b>Component-wise Quantization (CWQ)</b> considers the quantization sensitivity at each GNN component and applies different quantization bit to different components.In each layer, modern GNNs usually contain the attention component for measuring the relationship for each pair of nodes, and the combination component for computing the embedding for the next layer.Our key insight is that the attention component is more robust to the numerical error in the GNN quantization compared to the combination component. Thus,we can usually apply a lower quantization bit on the attention component than the combination component.</p>
									<p><b>Topology-aware Quantization (TAQ)</b> exploits the graph topology information and applies different quantization bits for different nodes based on their most essential topology property – degree. Nodes with a large degree are more robust to the quantization error and we can use smaller quantization bits to these high-degree nodes</p>
									<p><b>Layer-wise Quantization (LWQ)</b> exploits the diverse quantization sensitivity in individual GNN layers and provides different quantization bits to each layer.Under the same memory consumption, assigning higher bits to the leading layers generally leads to higher accuracy, compared to assigning higher bits to the succeeding layers.</p>
									<hr class="major" />

									

									<hr class="major" />

									<h2>Auto-bit Selection</h2>
									<p>There are three challenges in solving this combinatorial optimization problem. First, there is a large design space due to the abundant quantization granularity.Second, large diversity exists in the GNN model design in terms of the attention generation in the aggregation components and the neural network design in the combination components. Third, graph topology usually varies in terms of the number of nodes and the degree distribution, making the measurement of quantization intractable.To address these challenges, we build an <b>auto-bit selection (ABS)</b> with two main components: <b>a machine learning cost model</b> that predicts the accuracy of the quantized GNN under a given quantization configuration, and <b>an exploration scheme</b> to select the promising configurations.</p>
									<h3>Machine Learning Cost Model</h3>
									<img src="images/SGQuant_3.png" alt="" style="width: 400px; float: left;"/>
									<p><br />We build a ML cost model that learns on the fly the interaction among quantization bits, GNN models, and the graph topology. We treat this task as a regression problem and use a traditional ML model — regression tree, as our ML cost model. We prefer the regression tree over neural networks since the former one has faster inference speed and does not require a large amount of training data.</p>
									<h3>Exploration Scheme</h3>
									<p>We propose an exploration scheme that iteratively trains the ML cost model and selects promising configurations. In this way, we can balance the low overhead in training the ML cost model and the precise prediction of configuration accuracies. In particular, there are five steps in our exploration scheme.</p>
									<p1>• <b>Step1</b>: Randomly select a small number N<sub>mea</sub> of configurations, extract features, and measure their accuracies.<br>
										• <b>Step2</b>: Train the ML cost model based on the collected features and labels.<br>
										• <b>Step3</b>: Sample a large number N<sub>sample</sub> of configurations, use the ML cost model to predict their accuracy,<br> &emsp; &emsp;&emsp;&ensp;and find the ones with the top-N<sub>mea</sub> accuracy.<br>
										• <b>Step4</b>: Extract features of the selected configurations and measure their accuracies.<br>
										• <b>Step5</b>: Repeat <b>Step2</b> - <b>Step4</b> until reaching N<sub>iter</sub> iterations.</p1>
								</section>

						</div>
					</div>

				<!-- Sidebar -->
					<div id="sidebar">
						<div class="inner">

							<!-- Search -->
								<section id="search" class="alt">
									<form method="post" action="#">
										<input type="text" name="query" id="query" placeholder="Search" />
									</form>
								</section>

							<!-- Menu -->
								<nav id="menu">
									<header class="major">
										<h2>Menu</h2>
									</header>
									<ul>
										<li><a href="index.html">Homepage</a></li>
										<!-- <li>
											<span class="opener">Research</span> 
										<ul>  -->
										<li><a href="InformationField.html">Information Field</a></li>
										<li><a href="DSXplore.html">DSXplore</a></li>
										<li><a href="SGQuant.html">SGQuant</a></li>
										<li><a href="MDGAN.html">MDGAN</a></li>
										<!-- </ul>
										</li> -->
								
									</ul>
								</nav>

							<!-- Section -->
								<!-- <section>
									<header class="major">
										<h2>Ante interdum</h2>
									</header>
									<div class="mini-posts">
										<article>
											<a href="#" class="image"><img src="images/pic07.jpg" alt="" /></a>
											<p>Aenean ornare velit lacus, ac varius enim lorem ullamcorper dolore aliquam.</p>
										</article>
										<article>
											<a href="#" class="image"><img src="images/pic08.jpg" alt="" /></a>
											<p>Aenean ornare velit lacus, ac varius enim lorem ullamcorper dolore aliquam.</p>
										</article> -->
										<!-- <article>
											<a href="#" class="image"><img src="images/pic09.jpg" alt="" /></a>
											<p>Aenean ornare velit lacus, ac varius enim lorem ullamcorper dolore aliquam.</p>
										</article>
									</div>
									<ul class="actions">
										<li><a href="#" class="button">More</a></li>
									</ul>
								</section> -->

							<!-- Section -->
							<section>
								<header class="major">
									<h2>Get in touch</h2>
								</header>
								<!-- <p>Sed varius enim lorem ullamcorper dolore aliquam aenean ornare velit lacus, ac varius enim lorem ullamcorper dolore. Proin sed aliquam facilisis ante interdum. Sed nulla amet lorem feugiat tempus aliquam.</p> -->
								<ul class="contact">
									<li class="icon solid fa-envelope"><a href="Ameliapxq0131@gmail.com">Ameliapxq0131@gmail.com</ameliapxq0131@gmail></a></li>
									<!-- <li class="icon solid fa-phone">(000) 000-0000</li> -->
									<li class="icon solid fa-home">No.4,Section 2,North Jianshe Road,<br />
										Chengdu,P.R.China,<br /> Zip/Postal Code:610054</li>
								</ul>
							</section>

							<!-- Footer -->
								<footer id="footer">
									<!-- <p class="copyright">&copy; Untitled. All rights reserved. Demo Images: <a href="https://unsplash.com">Unsplash</a>. Design: <a href="https://html5up.net">HTML5 UP</a>.</p> -->
								</footer>

						</div>
					</div>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
